train_dataset:
  streaming_c4:
    remote: s3://allenai-c4/mds/1/
    local: /tmp/mds-cache/mds-c4/
    split: val
    shuffle: true
    tokenizer_name: gpt2
    max_seq_len: 2048
    group_method: truncate

val_dataset:
  streaming_c4:
    remote: s3://allenai-c4/mds/1/
    local: /tmp/mds-cache/mds-c4/
    split: val
    shuffle: false
    tokenizer_name: gpt2
    max_seq_len: 2048
    group_method: truncate

model:
  gpt2:
    use_pretrained: false
    tokenizer_name: gpt2
    gradient_checkpointing: false
    model_config:
      attn_pdrop: 0.0
      embd_pdrop: 0.0
      resid_pdrop: 0.0
      summary_first_dropout: 0.0
      initializer_range: 0.02
      layer_norm_epsilon: 1.0e-05
      n_embd: 2048
      n_head: 1
      n_layer: 24
      n_positions: 2048
      use_cache: false
optimizers:
  decoupled_adamw:
    lr: 2.0e-4
    betas:
      - 0.9
      - 0.95
    eps: 1.0e-08
    weight_decay: 0.0
schedulers:
  - cosine_decay_with_warmup:
      t_warmup: 100ba
max_duration: 100ba
train_batch_size: 512
grad_accum: 32
eval_batch_size: 64
seed: 17
dataloader:
  pin_memory: true
  persistent_workers: true
  num_workers: 8
  timeout: 0
  prefetch_factor: 2
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
eval_interval: 1000ba

precision: bf16
fsdp: true
fsdp_min_params: 5e7
fsdp_sharding_strategy: FULL_SHARD 

log_to_console: true
console_log_level: BATCH
callbacks:
  lr_monitor: {}
  speed_monitor:
    window_size: 1

#loggers:
#  wandb:
#    project: abhi-fsdp

run_name: gpt3-1B-h1-msl-2048-fsdp-5e7-safe-bf16-tbs-512-ga-32-gpus-8-40gb
